{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "        make_scorer,\n",
    "        confusion_matrix, \n",
    "        cohen_kappa_score, \n",
    "        accuracy_score, \n",
    "        precision_score, \n",
    "        recall_score, \n",
    "        f1_score, \n",
    "        roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier # decision trees for classification\n",
    "from sklearn.neural_network import  MLPClassifier # neural networks for classification\n",
    "from sklearn.naive_bayes import GaussianNB # naive bayes for classification\n",
    "from sklearn.svm import SVC # support vector machines for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = {\n",
    "        \"accuracy\": make_scorer(accuracy_score),\n",
    "        \"precision\": make_scorer(precision_score),\n",
    "        \"recall\": make_scorer(recall_score),\n",
    "        \"f1\": make_scorer(f1_score),\n",
    "        \"AUC\": make_scorer(roc_auc_score, needs_proba=True),\n",
    "        \"specificity\": make_scorer(specificity_score),\n",
    "        \"kappa\":make_scorer(cohen_kappa_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload and divide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    936640.0\n",
       "mean          0.0\n",
       "std           0.0\n",
       "min           0.0\n",
       "25%           0.0\n",
       "50%           0.0\n",
       "75%           0.0\n",
       "max           0.0\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d = pd.read_csv(\"dados/06-categoricas.csv\")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data? faster models\n",
    "#d = d.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = d.drop(\"target\", axis=1), d[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare for cross validation and/or train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation splitting strategy\n",
    "splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "# https://towardsdatascience.com/understanding-the-confusion-matrix-and-how-to-implement-it-in-python-319202e0fe4d\n",
    "test_size = 0.33\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = test_size, random_state=1234, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=25, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validate \\\n",
    "scenarios 1, 2, 3, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>test_specificity</th>\n",
       "      <th>test_kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.359414</td>\n",
       "      <td>0.110348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_accuracy  test_precision  test_recall  test_f1  \\\n",
       "0  0.359414    0.110348            NaN             NaN          NaN      NaN   \n",
       "\n",
       "   test_AUC  test_specificity  test_kappa  \n",
       "0       NaN               NaN         NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(dt, X, y, cv=splitter, scoring=METRICS)\n",
    "dt_scores = pd.DataFrame(scores)\n",
    "pd.DataFrame(dt_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20436\\1143791591.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mypred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdt_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_f1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "dt.fit(xtrain, ytrain)\n",
    "ypred = dt.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "dt_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(dt_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(50,50), max_iter=20, random_state=1234)\n",
    "scores_nn = cross_validate(nn, X, y, cv=splitter, scoring=METRICS)\n",
    "nn_scores = pd.DataFrame(scores_nn)\n",
    "pd.DataFrame(nn_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### gaussian naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "scores_nb = cross_validate(nb, X, y, cv=splitter, scoring=METRICS)\n",
    "nb_scores = pd.DataFrame(scores_nb)\n",
    "pd.DataFrame(nb_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### support vector classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state=1234, probability=True)\n",
    "scores_svm = cross_validate(svm, X, y, cv=splitter, scoring=METRICS)\n",
    "svm_scores = pd.DataFrame(scores_svm)\n",
    "pd.DataFrame(svm_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outros modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs.fit(xtrain)\n",
    "ypred = nbrs.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "nbrs_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(nbrs_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### nearest centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "nc = NearestCentroid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs.fit(xtrain)\n",
    "nc = nc.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "nc_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(nc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=20, max_depth=None, min_samples_split=2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_clf = cross_validate(clf, X, y, cv=splitter, scoring=METRICS)\n",
    "clf_scores = pd.DataFrame(scores_clf)\n",
    "pd.DataFrame(clf_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(xtrain, ytrain)\n",
    "ypred = clf.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "clf_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(clf_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ada = cross_validate(ada, X, y, cv=splitter, scoring=METRICS)\n",
    "ada_scores = pd.DataFrame(scores_ada)\n",
    "pd.DataFrame(ada_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada.fit(xtrain, ytrain)\n",
    "ypred = ada.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "ada_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(ada_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=250, learning_rate=1.0, max_depth=20, random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_gbc = cross_validate(gbc, X, y, cv=splitter, scoring=METRICS)\n",
    "gbc_scores = pd.DataFrame(scores_gbc)\n",
    "pd.DataFrame(gbc_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.fit(xtrain, ytrain)\n",
    "ypred = gbc.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "gbc_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(gbc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### tpot automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.1, max_depth=25, min_child_weight=16, n_estimators=100, n_jobs=1, subsample=0.8500000000000001, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_xgb = cross_validate(xgb, X, y, cv=splitter, scoring=METRICS)\n",
    "xgb_scores = pd.DataFrame(scores_xgb)\n",
    "pd.DataFrame(xgb_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split \\\n",
    "SCENARIO 6 WINNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-map:1.00000\n",
      "[1]\tvalidation_0-map:1.00000\n",
      "[2]\tvalidation_0-map:1.00000\n",
      "[3]\tvalidation_0-map:1.00000\n",
      "[4]\tvalidation_0-map:1.00000\n",
      "[5]\tvalidation_0-map:1.00000\n",
      "[6]\tvalidation_0-map:1.00000\n",
      "[7]\tvalidation_0-map:1.00000\n",
      "[8]\tvalidation_0-map:1.00000\n",
      "[9]\tvalidation_0-map:1.00000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ypred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20436\\4146443315.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'map'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#ypred = xgb.predict(xtest)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ypred' is not defined"
     ]
    }
   ],
   "source": [
    "# train/test\n",
    "xgb.fit(xtrain, ytrain)\n",
    "ypred = xgb.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### https://www.youtube.com/watch?v=GrJP9FLV3FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(objective = 'binary:logistic',seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_xgb = cross_validate(xgb, X, y, cv=splitter, scoring=METRICS)\n",
    "xgb_scores = pd.DataFrame(scores_xgb)\n",
    "pd.DataFrame(xgb_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split \\\n",
    "SCENARIO 7 WINNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-map:1.00000\n",
      "[1]\tvalidation_0-map:1.00000\n",
      "[2]\tvalidation_0-map:1.00000\n",
      "[3]\tvalidation_0-map:1.00000\n",
      "[4]\tvalidation_0-map:1.00000\n",
      "[5]\tvalidation_0-map:1.00000\n",
      "[6]\tvalidation_0-map:1.00000\n",
      "[7]\tvalidation_0-map:1.00000\n",
      "[8]\tvalidation_0-map:1.00000\n",
      "[9]\tvalidation_0-map:1.00000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ypred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20436\\4146443315.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'map'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#ypred = xgb.predict(xtest)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ypred' is not defined"
     ]
    }
   ],
   "source": [
    "# train/test\n",
    "xgb.fit(xtrain, ytrain, verbose=True, early_stopping_rounds=10, eval_metric='map', eval_set=[(xtest,ytest)])\n",
    "#ypred = xgb.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### https://www.youtube.com/watch?v=ap2SS0-XPcE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgboost = XGBClassifier(learning_rate=0.1,\n",
    "                                      max_depth=5,\n",
    "                                      n_estimators=5000,\n",
    "                                      subsample=0.5,\n",
    "                                      colsample_bytree=0.5,\n",
    "                                      eval_metric='map',\n",
    "                                      verbosity=1)\n",
    "\n",
    "eval_set = [(xtest, ytest)]\n",
    "\n",
    "model_xgboost.fit(xtrain,\n",
    "                  ytrain,\n",
    "                  early_stopping_rounds=10,\n",
    "                  eval_set=eval_set,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model_xgboost.predict(xtrain)\n",
    "y_valid_pred = model_xgboost.predict(xtest)\n",
    "\n",
    "#print(\"MAP Train: {:.4f}\\nMAP Valid: {:.4f}\".format(confusion_matrix(ytrain, y_train_pred),\n",
    "#                                                    confusion_matrix(ytest, y_valid_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(ytest,y_valid_pred)\n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params list to be testes in gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = [0.02, 0.05, 0.1]\n",
    "max_depth_list = [2, 3, 5]\n",
    "n_estimators_list = [1000, 2000, 3000]\n",
    "\n",
    "params_dict = {\"learning_rate\": learning_rate_list,\n",
    "               \"max_depth\": max_depth_list,\n",
    "               \"n_estimators\": n_estimators_list}\n",
    "\n",
    "num_combinations = 1\n",
    "for v in params_dict.values(): num_combinations *= len(v) \n",
    "\n",
    "print(num_combinations)\n",
    "params_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_f1_score(model, X, y): return f1_score(y, model.predict(X))\n",
    "\n",
    "model_xgboost_hp = GridSearchCV(estimator=XGBClassifier(subsample=0.5,\n",
    "                                                                colsample_bytree=0.25,\n",
    "                                                                eval_metric='map',\n",
    "                                                                use_label_encoder=False),\n",
    "                                param_grid=params_dict,\n",
    "                                cv=2,\n",
    "                                scoring=my_f1_score,\n",
    "                                return_train_score=True,\n",
    "                                verbose=4)\n",
    "\n",
    "model_xgboost_hp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results = pd.DataFrame(model_xgboost_hp.cv_results_)\n",
    "df_cv_results = df_cv_results[['rank_test_score','mean_test_score','mean_train_score',\n",
    "                               'param_learning_rate', 'param_max_depth', 'param_n_estimators']]\n",
    "df_cv_results.sort_values(by='rank_test_score', inplace=True)\n",
    "df_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First sort by number of estimators as that would be x-axis\n",
    "df_cv_results.sort_values(by='param_n_estimators', inplace=True)\n",
    "\n",
    "# Find values of AUC for learning rate of 0.05 and different values of depth\n",
    "lr_d2 = df_cv_results.loc[(df_cv_results['param_learning_rate']==0.05) & (df_cv_results['param_max_depth']==2),:]\n",
    "lr_d3 = df_cv_results.loc[(df_cv_results['param_learning_rate']==0.05) & (df_cv_results['param_max_depth']==3),:]\n",
    "lr_d5 = df_cv_results.loc[(df_cv_results['param_learning_rate']==0.05) & (df_cv_results['param_max_depth']==5),:]\n",
    "lr_d7 = df_cv_results.loc[(df_cv_results['param_learning_rate']==0.05) & (df_cv_results['param_max_depth']==7),:]\n",
    "\n",
    "# Let us plot now\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "lr_d2.plot(x='param_n_estimators', y='mean_test_score', label='Depth=2', ax=ax)\n",
    "lr_d3.plot(x='param_n_estimators', y='mean_test_score', label='Depth=3', ax=ax)\n",
    "lr_d5.plot(x='param_n_estimators', y='mean_test_score', label='Depth=5', ax=ax)\n",
    "lr_d7.plot(x='param_n_estimators', y='mean_test_score', label='Depth=7', ax=ax)\n",
    "plt.ylabel('Mean Validation AUC')\n",
    "plt.title('Performance wrt # of Trees and Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First sort by learning rate as that would be x-axis\n",
    "df_cv_results.sort_values(by='param_learning_rate', inplace=True)\n",
    "\n",
    "# Find values of AUC for learning rate of 0.05 and different values of depth\n",
    "lr_t3k_d2 = df_cv_results.loc[(df_cv_results['param_n_estimators']==3000) & (df_cv_results['param_max_depth']==2),:]\n",
    "\n",
    "# Let us plot now\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "lr_t3k_d2.plot(x='param_learning_rate', y='mean_test_score', label='Depth=2, Trees=3000', ax=ax)\n",
    "plt.ylabel('Mean Validation AUC')\n",
    "plt.title('Performance wrt learning rate')\n",
    "\n",
    "# rank_test_score\tmean_test_score\tmean_train_score\tparam_learning_rate\tparam_max_depth\tparam_n_estimators\n",
    "# 1\t0.704338\t0.722704\t0.1\t5\t2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgboost_fin = XGBClassifier(learning_rate=0.1,\n",
    "                                          max_depth=5,\n",
    "                                          n_estimators=2000,\n",
    "                                          subsample=0.5,\n",
    "                                          colsample_bytree=0.25,\n",
    "                                          eval_metric='map',\n",
    "                                          verbosity=1,\n",
    "                                          use_label_encoder=False)\n",
    "\n",
    "# Passing both training and validation dataset as we want to plot AUC for both\n",
    "eval_set = [(xtrain, ytrain),(xtest, ytest)]\n",
    "\n",
    "model_xgboost_fin.fit(xtrain,\n",
    "                  ytrain,\n",
    "                  early_stopping_rounds=20,\n",
    "                  eval_set=eval_set,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other models from tpot automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from xgboost import XGBClassifier\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exported_pipeline = make_pipeline(\n",
    "#    StackingEstimator(estimator=XGBClassifier(learning_rate=0.1, max_depth=8, min_child_weight=16, n_estimators=100, n_jobs=1, subsample=0.8500000000000001, verbosity=0)),\n",
    "#    StackingEstimator(estimator=RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.4, min_samples_leaf=9, min_samples_split=18, n_estimators=100)),\n",
    "#    MultinomialNB(alpha=1.0, fit_prior=False)\n",
    "#)\n",
    "exported_pipeline = make_pipeline(\n",
    "    MaxAbsScaler(),\n",
    "    StackingEstimator(estimator=RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_features=0.45, min_samples_leaf=17, min_samples_split=10, n_estimators=100)),\n",
    "    XGBClassifier(learning_rate=0.01, max_depth=2, min_child_weight=7, n_estimators=100, n_jobs=1, subsample=0.45, verbosity=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline.fit(xtrain, ytrain)\n",
    "ypred = exported_pipeline.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred)\n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "pipe_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(pipe_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = exported_pipeline.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(ytest,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "pipe_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(pipe_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randomized search + xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "model = XGBClassifier()\n",
    "param_vals = {'max_depth': [200, 500, 800, 1100], 'n_estimators': [100, 200, 300, 400],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 1, 10]}\n",
    "random_rf = RandomizedSearchCV(estimator=model, param_distributions=param_vals,\n",
    "                              n_iter=10, scoring='f1', cv=5,\n",
    "                              refit=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and prediction\n",
    "random_rf.fit(xtrain, ytrain)\n",
    "preds = random_rf.best_estimator_.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensembles\n",
    "- https://towardsdatascience.com/ensemble-learning-using-scikit-learn-85c4531ff86a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix asap\n",
    "d = d.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = d.drop(\"target\", axis=1), d[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new a knn model\n",
    "knn = KNeighborsClassifier()\n",
    "#create a dictionary of all values we want to test for n_neighbors\n",
    "params_knn = {'n_neighbors': np.arange(1, 25)}\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
    "#fit model to training data\n",
    "knn_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best model\n",
    "knn_best = knn_gs.best_estimator_\n",
    "#check best n_neigbors value\n",
    "print(knn_gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "#create a dictionary of all values we want to test for n_estimators\n",
    "params_rf = {'n_estimators': [50, 100, 200]}\n",
    "#use gridsearch to test all values for n_estimators\n",
    "rf_gs = GridSearchCV(rf, params_rf, cv=5)\n",
    "#fit model to training data\n",
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best model\n",
    "rf_best = rf_gs.best_estimator_\n",
    "#check best n_estimators value\n",
    "print(rf_gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "#fit the model to the training data\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the three models with the test data and print their accuracy scores\n",
    "print('knn: {}'.format(knn_best.score(X_test, y_test)))\n",
    "print('rf: {}'.format(rf_best.score(X_test, y_test)))\n",
    "print('log_reg: {}'.format(log_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Voting Classifier\n",
    "joined result of the prior 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "#create a dictionary of our models\n",
    "estimators=[('knn', knn_best), ('rf', rf_best), ('log_reg', log_reg)]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model to training data\n",
    "ensemble.fit(X_train, y_train)\n",
    "#test our model on the test data\n",
    "ensemble.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = ensemble.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred)\n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "ensemble_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(ensemble_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "941a02c14a17852a9cb07cad892c4a36ec1d8a3f93fc8ad448615ff7fbd85d7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
