{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "        make_scorer,\n",
    "        confusion_matrix, \n",
    "        cohen_kappa_score, \n",
    "        accuracy_score, \n",
    "        precision_score, \n",
    "        recall_score, \n",
    "        f1_score, \n",
    "        roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier # decision trees for classification\n",
    "from sklearn.neural_network import  MLPClassifier # neural networks for classification\n",
    "from sklearn.naive_bayes import GaussianNB # naive bayes for classification\n",
    "from sklearn.svm import SVC # support vector machines for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = {\n",
    "        \"accuracy\": make_scorer(accuracy_score),\n",
    "        \"precision\": make_scorer(precision_score),\n",
    "        \"recall\": make_scorer(recall_score),\n",
    "        \"f1\": make_scorer(f1_score),\n",
    "        \"AUC\": make_scorer(roc_auc_score, needs_proba=True),\n",
    "        \"specificity\": make_scorer(specificity_score),\n",
    "        \"kappa\":make_scorer(cohen_kappa_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload and divide data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = pd.read_csv(\"data/03-pico-sem-town-shortname.csv\")\n",
    "#d = pd.read_csv(\"data/03-idf.csv\")\n",
    "dint = pd.read_csv(\"data/top-clients.csv\")\n",
    "#d = pd.read_csv(\"data/top-clients.csv\")\n",
    "#d = d.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = d.drop(\"target\", axis=1), d[\"target\"]\n",
    "#Xi, yi = dint.drop(\"target\", axis=1), dint[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare for cross validation and/or train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation splitting strategy\n",
    "splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "# https://towardsdatascience.com/understanding-the-confusion-matrix-and-how-to-implement-it-in-python-319202e0fe4d\n",
    "test_size = 0.33\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = test_size, random_state=1234, stratify=y)\n",
    "#it, xiv, yit, yiv = train_test_split(Xi, yi, test_size = test_size, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=25, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>test_specificity</th>\n",
       "      <th>test_kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081314</td>\n",
       "      <td>0.013906</td>\n",
       "      <td>0.824156</td>\n",
       "      <td>0.79216</td>\n",
       "      <td>0.794376</td>\n",
       "      <td>0.793108</td>\n",
       "      <td>0.820455</td>\n",
       "      <td>0.846119</td>\n",
       "      <td>0.640223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_accuracy  test_precision  test_recall   test_f1  \\\n",
       "0  0.081314    0.013906       0.824156         0.79216     0.794376  0.793108   \n",
       "\n",
       "   test_AUC  test_specificity  test_kappa  \n",
       "0  0.820455          0.846119    0.640223  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(dt, X, y, cv=splitter, scoring=METRICS)\n",
    "dt_scores = pd.DataFrame(scores)\n",
    "pd.DataFrame(dt_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8257709683755647\n"
     ]
    }
   ],
   "source": [
    "dt.fit(xtrain, ytrain)\n",
    "ypred = dt.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "dt_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(dt_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(50,50), max_iter=20, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_nn = cross_validate(nn, X, y, cv=splitter, scoring=METRICS)\n",
    "nn_scores = pd.DataFrame(scores_nn)\n",
    "pd.DataFrame(nn_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(xtrain, ytrain)\n",
    "ypred = nn.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "nn_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(nn_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## gaussian naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "scores_nb = cross_validate(nb, X, y, cv=splitter, scoring=METRICS)\n",
    "nb_scores = pd.DataFrame(scores_nb)\n",
    "pd.DataFrame(nb_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## support vector classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state=1234, probability=True)\n",
    "scores_svm = cross_validate(svm, X, y, cv=splitter, scoring=METRICS)\n",
    "svm_scores = pd.DataFrame(scores_svm)\n",
    "pd.DataFrame(svm_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs.fit(xtrain)\n",
    "ypred = nbrs.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "nbrs_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(nbrs_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## nearest centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "nc = NearestCentroid()\n",
    "scores_nc = cross_validate(nc, X, y, cv=splitter, scoring=METRICS)\n",
    "clf_scores = pd.DataFrame(scores_nc)\n",
    "pd.DataFrame(nc_scores.mean()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=20, max_depth=None, min_samples_split=2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_clf = cross_validate(clf, X, y, cv=splitter, scoring=METRICS)\n",
    "clf_scores = pd.DataFrame(scores_clf)\n",
    "pd.DataFrame(clf_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(xtrain, ytrain)\n",
    "ypred = clf.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "clf_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(clf_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ada = cross_validate(ada, X, y, cv=splitter, scoring=METRICS)\n",
    "ada_scores = pd.DataFrame(scores_ada)\n",
    "pd.DataFrame(ada_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada.fit(xtrain, ytrain)\n",
    "ypred = ada.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "ada_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(ada_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=250, learning_rate=1.0, max_depth=20, random_state=0).fit(X, y)\n",
    "scores_gbc = cross_validate(gbc, X, y, cv=splitter, scoring=METRICS)\n",
    "gbc_scores = pd.DataFrame(scores_gbc)\n",
    "pd.DataFrame(gbc_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tpot automl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## xgboost\n",
    "    - https://www.youtube.com/watch?v=GrJP9FLV3FE&t=2807s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(objective = 'binary:logistic',seed=42)\n",
    "#learning_rate=0.1, max_depth=25, min_child_weight=16, n_estimators=100, n_jobs=1, subsample=0.8500000000000001, verbosity=0,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>test_specificity</th>\n",
       "      <th>test_kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.299152</td>\n",
       "      <td>0.016853</td>\n",
       "      <td>0.841016</td>\n",
       "      <td>0.840176</td>\n",
       "      <td>0.772759</td>\n",
       "      <td>0.804974</td>\n",
       "      <td>0.911706</td>\n",
       "      <td>0.891363</td>\n",
       "      <td>0.671184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_accuracy  test_precision  test_recall   test_f1  \\\n",
       "0  0.299152    0.016853       0.841016        0.840176     0.772759  0.804974   \n",
       "\n",
       "   test_AUC  test_specificity  test_kappa  \n",
       "0  0.911706          0.891363    0.671184  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_xgb = cross_validate(xgb, X, y, cv=splitter, scoring=METRICS)\n",
    "xgb_scores = pd.DataFrame(scores_xgb)\n",
    "pd.DataFrame(xgb_scores.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-map:0.83386\n",
      "[1]\tvalidation_0-map:0.84573\n",
      "[2]\tvalidation_0-map:0.84995\n",
      "[3]\tvalidation_0-map:0.85140\n",
      "[4]\tvalidation_0-map:0.85294\n",
      "[5]\tvalidation_0-map:0.85395\n",
      "[6]\tvalidation_0-map:0.85473\n",
      "[7]\tvalidation_0-map:0.85691\n",
      "[8]\tvalidation_0-map:0.85892\n",
      "[9]\tvalidation_0-map:0.86243\n",
      "[10]\tvalidation_0-map:0.86264\n",
      "[11]\tvalidation_0-map:0.86416\n",
      "[12]\tvalidation_0-map:0.86487\n",
      "[13]\tvalidation_0-map:0.86558\n",
      "[14]\tvalidation_0-map:0.86742\n",
      "[15]\tvalidation_0-map:0.86957\n",
      "[16]\tvalidation_0-map:0.87003\n",
      "[17]\tvalidation_0-map:0.87023\n",
      "[18]\tvalidation_0-map:0.87101\n",
      "[19]\tvalidation_0-map:0.87276\n",
      "[20]\tvalidation_0-map:0.87300\n",
      "[21]\tvalidation_0-map:0.87322\n",
      "[22]\tvalidation_0-map:0.87387\n",
      "[23]\tvalidation_0-map:0.87465\n",
      "[24]\tvalidation_0-map:0.87502\n",
      "[25]\tvalidation_0-map:0.87520\n",
      "[26]\tvalidation_0-map:0.87558\n",
      "[27]\tvalidation_0-map:0.87603\n",
      "[28]\tvalidation_0-map:0.87579\n",
      "[29]\tvalidation_0-map:0.87658\n",
      "[30]\tvalidation_0-map:0.87705\n",
      "[31]\tvalidation_0-map:0.87722\n",
      "[32]\tvalidation_0-map:0.87770\n",
      "[33]\tvalidation_0-map:0.87783\n",
      "[34]\tvalidation_0-map:0.87799\n",
      "[35]\tvalidation_0-map:0.87890\n",
      "[36]\tvalidation_0-map:0.87957\n",
      "[37]\tvalidation_0-map:0.87895\n",
      "[38]\tvalidation_0-map:0.87862\n",
      "[39]\tvalidation_0-map:0.87901\n",
      "[40]\tvalidation_0-map:0.87866\n",
      "[41]\tvalidation_0-map:0.87947\n",
      "[42]\tvalidation_0-map:0.87930\n",
      "[43]\tvalidation_0-map:0.87917\n",
      "[44]\tvalidation_0-map:0.87928\n",
      "[45]\tvalidation_0-map:0.87941\n",
      "[46]\tvalidation_0-map:0.87969\n",
      "[47]\tvalidation_0-map:0.87967\n",
      "[48]\tvalidation_0-map:0.87984\n",
      "[49]\tvalidation_0-map:0.87996\n",
      "[50]\tvalidation_0-map:0.87988\n",
      "[51]\tvalidation_0-map:0.87952\n",
      "[52]\tvalidation_0-map:0.87936\n",
      "[53]\tvalidation_0-map:0.88022\n",
      "[54]\tvalidation_0-map:0.88066\n",
      "[55]\tvalidation_0-map:0.87986\n",
      "[56]\tvalidation_0-map:0.88030\n",
      "[57]\tvalidation_0-map:0.88009\n",
      "[58]\tvalidation_0-map:0.87986\n",
      "[59]\tvalidation_0-map:0.88010\n",
      "[60]\tvalidation_0-map:0.88022\n",
      "[61]\tvalidation_0-map:0.88066\n",
      "[62]\tvalidation_0-map:0.88069\n",
      "[63]\tvalidation_0-map:0.88086\n",
      "[64]\tvalidation_0-map:0.88118\n",
      "[65]\tvalidation_0-map:0.88101\n",
      "[66]\tvalidation_0-map:0.88127\n",
      "[67]\tvalidation_0-map:0.88175\n",
      "[68]\tvalidation_0-map:0.88150\n",
      "[69]\tvalidation_0-map:0.88130\n",
      "[70]\tvalidation_0-map:0.88181\n",
      "[71]\tvalidation_0-map:0.88209\n",
      "[72]\tvalidation_0-map:0.88250\n",
      "[73]\tvalidation_0-map:0.88271\n",
      "[74]\tvalidation_0-map:0.88275\n",
      "[75]\tvalidation_0-map:0.88291\n",
      "[76]\tvalidation_0-map:0.88324\n",
      "[77]\tvalidation_0-map:0.88332\n",
      "[78]\tvalidation_0-map:0.88327\n",
      "[79]\tvalidation_0-map:0.88298\n",
      "[80]\tvalidation_0-map:0.88286\n",
      "[81]\tvalidation_0-map:0.88319\n",
      "[82]\tvalidation_0-map:0.88308\n",
      "[83]\tvalidation_0-map:0.88324\n",
      "[84]\tvalidation_0-map:0.88307\n",
      "[85]\tvalidation_0-map:0.88333\n",
      "[86]\tvalidation_0-map:0.88341\n",
      "[87]\tvalidation_0-map:0.88330\n",
      "[88]\tvalidation_0-map:0.88354\n",
      "[89]\tvalidation_0-map:0.88372\n",
      "[90]\tvalidation_0-map:0.88379\n",
      "[91]\tvalidation_0-map:0.88377\n",
      "[92]\tvalidation_0-map:0.88370\n",
      "[93]\tvalidation_0-map:0.88399\n",
      "[94]\tvalidation_0-map:0.88430\n",
      "[95]\tvalidation_0-map:0.88438\n",
      "[96]\tvalidation_0-map:0.88418\n",
      "[97]\tvalidation_0-map:0.88420\n",
      "[98]\tvalidation_0-map:0.88435\n",
      "[99]\tvalidation_0-map:0.88462\n",
      "0.869482496194825\n"
     ]
    }
   ],
   "source": [
    "# train/test\n",
    "xgb.fit(xtrain, ytrain, verbose=True, early_stopping_rounds=10, eval_metric='map', eval_set=[(xtest,ytest)])\n",
    "ypred = xgb.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meu xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.1, max_depth=25, min_child_weight=16, n_estimators=100, n_jobs=1, subsample=0.8500000000000001, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_xgb = cross_validate(xgb, X, y, cv=splitter, scoring=METRICS)\n",
    "xgb_scores = pd.DataFrame(scores_xgb)\n",
    "pd.DataFrame(xgb_scores.mean()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test\n",
    "xgb.fit(xtrain, ytrain)\n",
    "ypred = xgb.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from xgboost import XGBClassifier\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exported_pipeline = make_pipeline(\n",
    "#    StackingEstimator(estimator=XGBClassifier(learning_rate=0.1, max_depth=8, min_child_weight=16, n_estimators=100, n_jobs=1, subsample=0.8500000000000001, verbosity=0)),\n",
    "#    StackingEstimator(estimator=RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.4, min_samples_leaf=9, min_samples_split=18, n_estimators=100)),\n",
    "#    MultinomialNB(alpha=1.0, fit_prior=False)\n",
    "#)\n",
    "exported_pipeline = make_pipeline(\n",
    "    MaxAbsScaler(),\n",
    "    StackingEstimator(estimator=RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_features=0.45, min_samples_leaf=17, min_samples_split=10, n_estimators=100)),\n",
    "    XGBClassifier(learning_rate=0.01, max_depth=2, min_child_weight=7, n_estimators=100, n_jobs=1, subsample=0.45, verbosity=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline.fit(xtrain, ytrain)\n",
    "ypred = exported_pipeline.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred)\n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "pipe_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(pipe_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = exported_pipeline.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(ytest,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "pipe_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(pipe_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# param optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "model = XGBClassifier()\n",
    "param_vals = {'max_depth': [200, 500, 800, 1100], 'n_estimators': [100, 200, 300, 400],\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 1, 10]}\n",
    "random_rf = RandomizedSearchCV(estimator=model, param_distributions=param_vals,\n",
    "                              n_iter=10, scoring='f1', cv=5,\n",
    "                              refit=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and prediction\n",
    "random_rf.fit(xtrain, ytrain)\n",
    "preds = random_rf.best_estimator_.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(ytest,ypred) \n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "short_name                         float64\n",
       "town                               float64\n",
       "product_id                         float64\n",
       "client_id                          float64\n",
       "route_id                           float64\n",
       "sales_channel_id                   float64\n",
       "week_number                          int64\n",
       "weight                               int64\n",
       "pieces                             float64\n",
       "state_Top10                          int64\n",
       "state_Top11-21                       int64\n",
       "state_Top22-32                       int64\n",
       "client_name_Bimbo Store              int64\n",
       "client_name_Consignment              int64\n",
       "client_name_Eatery                   int64\n",
       "client_name_Fresh Market             int64\n",
       "client_name_General Market/Mart      int64\n",
       "client_name_Hospital/Pharmacy        int64\n",
       "client_name_Individual               int64\n",
       "client_name_NO IDENTIFICADO          int64\n",
       "client_name_Oxxo Store               int64\n",
       "client_name_Post                     int64\n",
       "client_name_School                   int64\n",
       "client_name_Small Franchise          int64\n",
       "client_name_Supermarket              int64\n",
       "client_name_Walmart                  int64\n",
       "brand_BAR                            int64\n",
       "brand_BIM                            int64\n",
       "brand_CC                             int64\n",
       "brand_DH                             int64\n",
       "brand_GBI                            int64\n",
       "brand_JMX                            int64\n",
       "brand_KOD                            int64\n",
       "brand_LAR                            int64\n",
       "brand_LON                            int64\n",
       "brand_MLA                            int64\n",
       "brand_MP                             int64\n",
       "brand_MR                             int64\n",
       "brand_ORO                            int64\n",
       "brand_RIC                            int64\n",
       "brand_SAN                            int64\n",
       "brand_SL                             int64\n",
       "brand_SUA                            int64\n",
       "brand_SUN                            int64\n",
       "brand_THO                            int64\n",
       "brand_TR                             int64\n",
       "brand_WON                            int64\n",
       "weight-pieces                      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensembles\n",
    "- https://towardsdatascience.com/ensemble-learning-using-scikit-learn-85c4531ff86a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix asap\n",
    "d = d.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = d.drop(\"target\", axis=1), d[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new a knn model\n",
    "knn = KNeighborsClassifier()\n",
    "#create a dictionary of all values we want to test for n_neighbors\n",
    "params_knn = {'n_neighbors': np.arange(1, 25)}\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
    "#fit model to training data\n",
    "knn_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best model\n",
    "knn_best = knn_gs.best_estimator_\n",
    "#check best n_neigbors value\n",
    "print(knn_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "#create a dictionary of all values we want to test for n_estimators\n",
    "params_rf = {'n_estimators': [50, 100, 200]}\n",
    "#use gridsearch to test all values for n_estimators\n",
    "rf_gs = GridSearchCV(rf, params_rf, cv=5)\n",
    "#fit model to training data\n",
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best model\n",
    "rf_best = rf_gs.best_estimator_\n",
    "#check best n_estimators value\n",
    "print(rf_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "#fit the model to the training data\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the three models with the test data and print their accuracy scores\n",
    "print('knn: {}'.format(knn_best.score(X_test, y_test)))\n",
    "print('rf: {}'.format(rf_best.score(X_test, y_test)))\n",
    "print('log_reg: {}'.format(log_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "#create a dictionary of our models\n",
    "estimators=[('knn', knn_best), ('rf', rf_best), ('log_reg', log_reg)]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model to training data\n",
    "ensemble.fit(X_train, y_train)\n",
    "#test our model on the test data\n",
    "ensemble.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = ensemble.predict(xtest)\n",
    "cm = confusion_matrix(ytest,ypred)\n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "ensemble_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(ensemble_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aaaaaaaaaaaaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.1, max_depth=20, max_features=0.3, min_samples_leaf=4, min_samples_split=15, n_estimators=100, subsample=0.9000000000000001)),\n",
    "    RobustScaler(),\n",
    "    StackingEstimator(estimator=GaussianNB()),\n",
    "    BernoulliNB(alpha=0.01, fit_prior=False)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline.fit(xtrain, ytrain)\n",
    "results = exported_pipeline.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(ytest,results)\n",
    "tp,fp,fn,tn = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "aa_f1 = (2*tp)/(2*tp+fp+fn)\n",
    "print(aa_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "941a02c14a17852a9cb07cad892c4a36ec1d8a3f93fc8ad448615ff7fbd85d7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
